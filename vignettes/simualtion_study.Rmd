---
title: "Vignette Title"
author: "Vignette Author"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

To explore the reliability of the technique, we will establish the model reliability with a short simulation study.  This study will deal with purely the Poisson implementation and look at the effect of sample size, number of competing models, the size of the parameter space, and the sample noise.

To do this, we first establish a routine for simualting data.  Each simulation comprises an input feature space, $X$ of *n* by *k* varibles drawn from a a standard normal distributon, an exposure (to check weight is being properly accounted for), and a series of randomly generated parameters, $B_1 .. B_n$, each drawn from a normal distribution with st dev. 0.5.  The input data is evenly split into $K$  classes (grouped by index in the data set) with each group possesing its own parameter set $B_K$.

The linear response, $ X B$ has an extra noise term added on prior to being converted to a $\lambda$ value by the $exp$ function.  The *rpois* function then links the exposure scaled $\lambda$ values to a Poisson variable.


``` {r}

library(emax.glm)
library(pROC)

generate_data <- function(n, k, noise = 1, K = 2){
  X <-  matrix(
    rnorm(n * k),
    ncol = k
  )
  exposure <- sample(10:30, n, replace=T)
  
  q <- floor(seq(1, n+1, n / K) - 1)
  
  B.list <- lapply(
    1:K,
    function(i) rnorm(k, 0, 0.5)
  )
  
  rho <-c(sapply(
      1:K,
      function(i) X[(q[i]+1):q[i + 1],] %*% B.list[[i]]
    ))
  
  class.id <- c(sapply(
    1:K,
    function(i) rep(i, q[i+1] - q[i])
  ))

  Y <- rpois(n, exposure * exp(rho + rnorm(n, 0, noise)))
  
  return (list(x = X, y = Y, exposure = exposure, B = B.list, class.id = class.id))
}

```

The general data processing flow is:

* Select an input space of *K* classes, *k* features, *n* observations, and *e* noise. 
* Fit a Poisson GLM to compare the EM model to a single effect model.
* Carry out 20 small EM GLM trials to find an optimal starting point.
* Select the optimal position from these trials, and runthe EM routine to convergence.


``` {r}
set.seed(1001)


{
  K <- 2
  k <- 5
  n <- 5000
  e <- 0.00001

  sim <- generate_data(n, k, noise = e, K = K)
  
  fit.glm <- glm(sim$y ~ sim$x -1, family=poisson())
  
  small.sim <- small.em(x = sim$x, y = sim$y, b.init = "random", weight = sim$exposure, K = K, debug = F, maxiter=25, maxiter.NR = 20, repeats = 20)
  
  params <- select_best(small.sim)
  fit.em <- em.glm(x=sim$x, y = sim$y, weight = sim$exposure, b.init = params, K = K, debug = F, param_errors = T)
  
}



```


The EM algorith may not fit models in the same oder as generated, so we check which is more likely by redividing the data by the known class.  With the pairing between fitted and simulated models found, we can then check consistency between known and fitted values and the average ROC values.

``` {r}

idx  <- lapply(
  1:2,
  function(i) which.max(apply(
    fit.em$class_probs[sim$class.id == i, ],
    2,
    sum
  ))
)

r <- sapply(
  1:K,
  function(i) roc(
    as.numeric(sim$class.id == 1),
    fit.em$class_probs[,i]
  )$auc
)


t.stats <- sapply(
  1:K,
  function(i) (sim$B[[idx[[i]] ]] - fit.em$params[[i]]) / fit.em$errors[[i]]
)


list(
  "Agrement" = mean(abs(t.stats) < qnorm(1 - 0.025 / (k*K))),
  "Ave. ROC" = mean(r)
)

```

Combining the simulation, fit and analysis into a sinlge function means we can perform a simulation to check for consistent performace:

``` {r}

K <- 2

simulation <- function(k = 5, n = 5000, e = 0.0001){
  tryCatch(
    {
      sim <- generate_data(n, k, noise = e, K = K)
      
      small.sim <- small.em(x = sim$x, y = sim$y, b.init = "random", weight = sim$exposure, K = K, debug = F, maxiter=25, maxiter.NR = 20, repeats = 20, noise=2)
      
      params <- select_best(small.sim)
      fit.em <- em.glm(x=sim$x, y = sim$y, weight = sim$exposure, b.init = params, K = K, debug = F, param_errors = T)
      
      idx  <- lapply(
        1:2,
        function(i) which.max(apply(
          fit.em$class_probs[sim$class.id == i, ],
          2,
          sum
        ))
      )
      
      r <- sapply(
        1:K,
        function(i) roc(
          as.numeric(sim$class.id == 1),
          fit.em$class_probs[,i]
        )$auc
      )
      
      t.stats <- sapply(
        1:K,
        function(i) (sim$B[[idx[[i]] ]] - fit.em$params[[i]]) / fit.em$errors[[i]]
      )
      
      return(list(
        "Agrement" = mean(abs(t.stats) < qnorm(1 - 0.025 / (k*K))),
        "Ave. ROC" = mean(r),
         "ll" = logLik(fit.em, x=sim$x, y=sim$y, weight = sim$exposure)
      ))
    },
    error = function(e) return(list(
      "Agrement" = "-",
      "Ave. ROC" = "-",
       "ll" = "-"
    ))
  )
}

a <- tryCatch(
  log(12),
  error = function(e) return (12)
)

a

```

