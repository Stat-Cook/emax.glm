---
title: "Introduction to 'em.glm' - first fits"
author: "R. M. Cook"
date: "5 March 2019"
output: html_document
---

---
title: "Vignette Title"
author: "Vignette Author"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Poisson EM workflow

The Expectation Maximization algorithm is used to find the (local) maximum likelihood parameters of a system with hidden variables.  Notably, it allows for the modelling of a mixture model by assuming that each observed data point has a corresponding unobserved feature corresponding to which model holds true.

Here we apply the EM approach to the general linear regression model - implementing a general Expectation Maximization linear model.

The main function of interest is the 'em.glm' function - it takes a matrix of co-variates (*x*), a target vector (*y*), a GLM family (defaulting to *poisson()*) and the number of target classes (*K*).  

Here we use some simulated data to demonstrate reproduction of the GLM functions results when dealing with a single class.  First the data:


``` {r, results = "asis"}

set.seed(4001)
library(emax.glm)
library(pander)
render <- pander::pandoc.table

df <- sim.1$data
true_parameters <- sim.1$params

render(head(df))
```

and then the fit:

``` {r , results = "asis"}

x <- as.matrix(df[, 1:5])
y <- df$y

fit.1 <- em.glm(
  x, y,
  family = poisson(),
  b.init = "random",
  K = 1,
  debug = F,
  param_errors = T
)


render(summary(fit.1))
```

The parameters, and errors are the same as for the standard GLM result:

``` {r, results = "asis"}

poisson.glm <- glm(
  y ~ -1 + .,
  family = poisson(),
  data = df
)

render(summary(poisson.glm)$coefficients)

```

Parameters, and parameter errors can be inspected using the *plot* command (adding reference values where necessary). 

```{r}

plot(fit.1, known_params = true_parameters)

```

The package includes both Pearson and deviance residuals for the Mixed Model GLM and can be accessed using the *residuals* call.   The current implementation requires the fitted parameters to be passed as arguments:


```{r, fig.width=6}

{
  par(mfrow=c(1,2))
  hist(residuals(poisson.glm, type = "deviance"), xlab="Deviance residual", main="")
  hist(residuals(fit.1, x = x, y = y, type = "deviance"), xlab="Deviance residual", main="")
}

```

In addition, several model quality parameters are implemented, including log-likelihood, BIC and model deviance:

``` {r, results = "asis"}
df <- data.frame(
  "Log likelihood" = c(logLik(fit.1, x=x, y=y), logLik(poisson.glm)),
  "Deviance" = c(deviance(fit.1, x=x, y=y), deviance(poisson.glm)),
  "BIC" = c(BIC(fit.1), BIC(poisson.glm))
)

rownames(df) <- c("emax.glm", "glm")

pander::pandoc.table(t(df))
```

To fit a multi-class model, increase K.  For example, for a data set comprised of two competing Poisson models (with the first and second half of the data arising from different parameters):

``` {r, results = "asis"}

set.seed(4001)
x.2 <- as.matrix(sim.2$data[,1:5])
y.2 <- c(sim.2$data$y)
p1 <- sim.2$p1
p2 <- sim.2$p2

fit.2 <- em.glm(
  x = x.2, y = y.2,
  family = poisson(),
  K = 2,
  b.init = "random",
  debug = F,
  param_errors = T
)

render(summary(fit.2))

```

And we can then compare the fitted parameters (and errors) to the known terms.

```{r, fig.height = 8}
{
  par(mfrow = c(2,1))
  plot(fit.2, known_params = p1)
  plot(fit.2, known_params = p2)
}
```

As well as checking how the observations are divided between the classes:

```{r, fig.height = 8}
plot_probabilities(fit.2)
```

Fitting higher order models with increasing numbers of classes can be done by further increasing *K*.

The superiority of the 'em.glm' fit over the single class GLM model is apparent by comparing the quality of fit metrics:

``` {r, results = "asis"}
poisson.glm.2 <- glm(
  y ~ -1 + .,
  family = poisson(),
  data = sim.2$data
)

df <- data.frame(
  "Log likelihood" = c(logLik(fit.2, x=x.2, y=y.2), logLik(poisson.glm.2)),
  "Deviance" = c(deviance(fit.2, x=x.2, y=y.2), deviance(poisson.glm.2)),
  "BIC" = c(BIC(fit.2), BIC(poisson.glm.2))
)

rownames(df) <- c("emax.glm", "glm")

render(t(df))
```

As well as greatly improving the normality of the deviance residuals:

``` {r, fig.height = 8}
{
  par(mfrow = c(2, 1))
  qqnorm(residuals(fit.2, x = x.2, y = y.2, type= "deviance"))
  legend("topleft", "EM GLM (K = 2)")
  qqnorm(residuals(poisson.glm.2, type= "deviance"))
  legend("topleft", "GLM")
}
```

