---
title: "Vignette Title"
author: "Vignette Author"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

The majority of work flows will use the 'em.glm' and 'small.em' commands.  The 'em.glm' deals with fitting a single model to the point of conversion while 'small.em' implements multiple short EM routines to find an optimal starting point.  The method for finding the global log-liklihood maxima out of the local log-likelihood maxima will remain an open question.

Here we have an example data set that was simualted as a mixture of Poisson models.  Each observation was generated from one of two Poisson processe in a 50:50 split, with the data consisting of 10 features over 5000 observations, including an offset term.  The standard GLM shows a poor fit of the data:

``` {r, message = F}
library(pander)
library(emax.glm)
library(AER)
render <- pander::pandoc.table

x <- sim.3$x
y <- sim.3$y
weight <- sim.3$exposure

pois.model <- glm(y ~ offset(log(weight)) + 1 + x, family = poisson())

disp.glm <- dispersiontest(pois.model)
``` 



``` {r, fig.width = 8, fig.height = 6}
qqnorm(residuals(pois.model, type = "deviance"))
``` 

With the data being markedly over-dispersed ($Disp_{GLM} =$ `r round(disp.glm$estimate, 2)`) and a rapid change in the deviance residuals.  Taking the EM appraoch, we first run a series of early stopping EM trials to find an optimal starting point.  Currently three approaches are implemented for generating starting poisitions of the EM fit:

* "radnom"" - generate *k* random values from a normal distribution centered at 0 with st. dev. 'noise' 
* "fit" - fit a GLM to the data, and then apply random noise  
* A list of values - A list of length *K* (number of classes) to start the fit from.
* A zero-argument function - if the user wishes to trial a different parameter generation technique.

The 'small.em' function takes the same arguments as 'em.glm' but allows for taking a sub-sample of the data set and early temination.  The argument 'sample.size' gives the absolute number of data points to sample, 'maxiter.NR' controls the maximum number of Newton-Raphson steps  to take, and 'repeats' sets the number of trails to attempt.   The greater the number of repeats, noise, sample size and number of iterations allowed the longer the algorithm will take, but the better the estimate of the parameter space. 


``` {r, results = "asis"}
warm.up <- small.em(
  x = x, y = y, weight = weight,
  K = 2,
  maxiter.NR = 25, repeats = 10, 
  sample.size = 1000, debug = F,
  b.init = "random", noise = 1)

render(summary(warm.up))
```

The 'select_best' function takes the optiaml model based on log-likelihood, and then a full data model can be fit: 

``` {r, fig.width = 8, fig.height = 6}
params <- select_best(warm.up)

fit.K2 <-  em.glm(
  x = x, y = y, weight = weight,
  K = 2, b.init = params,
  debug = F, param_errors = T
)

disp <- dispersion(fit.K2, x=x, y=y, weight=weight)

plot_probabilities(fit.K2, mar=rep(2,4))
``` 

The 2-class new model shows a good divide between the classes, with the top half and bottom half of the data divided between classes as it was simulated to be so.

``` {r, results = "asis"}
df <- data.frame(
  "Log likelihood" = c(logLik(fit.K2, x=x, y=y, weight = weight), logLik(pois.model)),
  "Deviance" = c(deviance(fit.K2, x=x, y=y, weight = weight), deviance(pois.model)),
  "BIC" = c(BIC(fit.K2), BIC(pois.model))
)

rownames(df) <- c("emax.glm", "glm")

render(t(df))
``` 

Comparing quality of fit metrics, it is clear the mixed model gives a far better fit, notably with a greatly reduced Pearson dispersion comapred to the GLM fit ($disp_{EM}$ = `r round(disp$pearson_dispersion, 3)`) and well distributed residuals:

``` {r, fig.width = 8, fig.height = 6}
{
  par(mfrow = c(2,1), mar = rep(2, 4))
  resid <- residuals(fit.K2, x=x, y=y, weight=weight, type="deviance")
  hist(resid, main="Deviance residuals")
  qqnorm(resid)
}
```

As well as giving good estimates of the parameters:

``` {r, fig.width = 8, fig.height = 6}
{
  par(mfrow = c(2,1), mar = rep(2, 4))
  plot(fit.K2, known_params = sim.3$p1)
  plot(fit.K2, known_params = sim.3$p2)
}
```

The question of how many classes should be fit to a data set will come down to many factors, but as a first step consider the BIC and dispersion scores as a function of the number of classes.  Overall we aim to find a model such that the Pearson dispersion is no less than 1, with the lowest BIC score available:

``` {r, fig.width = 8, fig.height = 6}

test_K <- function(K){
  warm.up <- small.em(  
    x = x, y = y, weight = weight,
    K = K,
    maxiter.NR = 20, repeats = 10, 
    sample.size = 500, debug = F,
    b.init = "random", noise = 1
  )
  
  params <- select_best(warm.up)
  
  fit <- em.glm(
      x = x, y = y, weight = weight,
    K = K, debug = F,
    b.init = params,
    param_errors = F
  )
  fit
}
  
models <- lapply(
  2:5,
  test_K
)  

{
  par(mfrow=c(2, 1))

  plot(
    2:5, 
    sapply(models, BIC),
    xlab = "No. of classes",
    ylab = "BIC score"
  )
  
  plot(
    2:5,
    sapply(models, function(i) dispersion(i, x=x, y=y, weight=weight)$pearson_dispersion),
    xlab = "No. of classes",
    ylab = "Pearson's Dispersion"
  )
}

```

